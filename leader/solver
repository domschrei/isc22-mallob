#!/usr/bin/env python3
from distutils.log import ERROR
import json
import logging
import os
import subprocess
import sys
import threading
import time
import re
import concurrent.futures
from threading import Timer
from concurrent.futures import ThreadPoolExecutor

import signal
from subprocess import Popen
from subprocess import TimeoutExpired
from typing import Callable
from enum import Enum


#######################################################

class PipelineMode(Enum):
    # Single machine with 64 hwthreads, sequential proof production and DRAT proof checking
    SINGLE_MACHINE_64HWT_SEQ_PPROD_AND_DRAT = 1
    # Single machine with 64 hwthreads, parallel proof production
    SINGLE_MACHINE_64HWT_PAR_PPROD = 2
    # 100 machines with 16 hwthreads, parallel proof production
    HUNDRED_MACHINES_16HWT_PAR_PPROD = 3

# TODO Change this to the desired mode of execution.
# pipeline_mode = PipelineMode.SINGLE_MACHINE_64HWT_PAR_PPROD
pipeline_mode = PipelineMode.SINGLE_MACHINE_64HWT_SEQ_PPROD_AND_DRAT


#######################################################


logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

def reverse_function(x):
    return x[::-1]

# total number of *local* slots (MPI processes) on this machine
def get_num_local_slots():
    return 4 # only for testing!!
    # if pipeline_mode == PipelineMode.HUNDRED_MACHINES_16HWT_PAR_PPROD:
    #     return 4 # distributed setup: 16/4 = 4 slots
    # else:
    #     return 16 # shared-memory setup: 64/4 = 16 slots

# total time to run in seconds.  This allows ~1 minute of gracetime before ATS would pull the plug.
TOTAL_TIME = 4940

# Class to help identify when a timeout has been triggered
class _TimeoutRecorder:
    __timed_out: bool = False

    def record_timeout(self):
        self.__timed_out = True

    def is_timed_out(self) -> bool:
        return self.__timed_out

class Runner:
    def __init__(self, request_directory: str):
        self.logger = logging.getLogger("Runner")
        self.logger.setLevel(logging.INFO)
        self.request_directory = request_directory
        os.environ['PYTHONUNBUFFERED'] = "1"
        self.sigterm_grace_seconds = 10

        # for sequential access to stdout/stderr files.
        self.stderr_lock = threading.Lock()
        self.stdout_lock = threading.Lock()

    def killpg_if_running(self, process: Popen, sigkill_grace_seconds: int, post_kill_hook: Callable[[], None] = None) -> None:
        self.__sendpg_signal(process, signal.SIGTERM, post_kill_hook)
        try:
            process.wait(sigkill_grace_seconds)
        except TimeoutExpired:
            self.__sendpg_signal(process, signal.SIGKILL)


    def __sendpg_signal(self, proc: Popen, proc_signal: signal.Signals, post_kill_hook: Callable[[], None] = None) -> None:
        try:
            pg_id = os.getpgid(proc.pid)
            self.logger.info("Sending %s to process group %d for process %d", proc_signal, pg_id, proc.pid)
            os.killpg(pg_id, proc_signal)
            if post_kill_hook:
                post_kill_hook()
        except ProcessLookupError:
            # Process not found, so we can assume it has already ended. This can happen because of
            # race-conditions around when the process finishes, this method is called, and timeout
            # cancelled
            self.logger.debug("No process group for process %d found. Ignoring %s.", proc.pid, proc_signal)

    # since we make the DRAT and LRAT proofs proceed in parallel, 
    # for this version of the file, we need locks to ensure sequential access.
    def process_stream(self, stream, str_name, file_handle, lock):
        line = stream.readline()
        while line != "":
            self.logger.info(f"{str_name}: {line}")
            with lock: 
                file_handle.write(line)
            # file_handle.flush()
            line = stream.readline()

    def run(self, cmd: list, timeout_seconds, is_append):
        self.logger.info("Running command: %s", str(cmd))
        
        stdout_target_loc = os.path.join(self.request_directory, "stdout.log")
        stderr_target_loc = os.path.join(self.request_directory, "stderr.log")
        
        access = "a" if is_append else "w"

        with open(stdout_target_loc, access) as stdout_handle:
            with open(stderr_target_loc, access) as stderr_handle:
                start = time.monotonic()
                proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                    universal_newlines=True, start_new_session=True)
                stdout_t = threading.Thread(target = self.process_stream, args=(proc.stdout, "STDOUT", stdout_handle, self.stdout_lock))
                stderr_t = threading.Thread(target = self.process_stream, args=(proc.stderr, "STDERR", stderr_handle, self.stderr_lock))
                stdout_t.start()
                stderr_t.start()
                timeout_recorder = _TimeoutRecorder()
                timeout_timer = Timer(
                    timeout_seconds + self.sigterm_grace_seconds,
                    lambda: self.killpg_if_running(process=proc,
                                                        # Arbitrarily picked to fail fast
                                                        sigkill_grace_seconds=3,
                                                        post_kill_hook=timeout_recorder.record_timeout))
                try: 
                    timeout_timer.start()
                    return_code = proc.wait()
                except Exception as e:
                    self.logger.info(f"Exception occurred during job run: {e}")
                    if not return_code:
                        return_code = -1
                finally:
                    timeout_timer.cancel()
                    stdout_t.join()
                    stderr_t.join()

                stop = time.monotonic()
                timing = stop-start
                timing_str = f">>>>>{cmd[0]} : {timing} seconds<<<<<\n"
                self.logger.info(f"STDERR: {timing_str}")
                stderr_handle.write(timing_str)
                
        return {
            "stdout": stdout_target_loc,
            "stderr": stderr_target_loc,
            "return_code": return_code,
            "time": timing,
            "timed_out": timeout_recorder.is_timed_out(),
            "output_directory": self.request_directory
        }


    def get_input_json(self):
        input = os.path.join(self.request_directory, "input.json")
        with open(input) as f:
            return json.loads(f.read())
    
    def create_hostfile(self,ips, request_dir):
        hostfile_path = os.path.join(request_dir, 'combined_hostfile')
        with open(hostfile_path, 'w+') as f:
            for ip in ips:
                f.write(f'{ip} slots={get_num_local_slots()}')
                f.write('\n')
            return hostfile_path

    def get_command(self, input_json):
        problem_path = input_json.get("problem_path")
        worker_node_ips = input_json.get("worker_node_ips", [])
        
        combined_hostfile = self.create_hostfile(worker_node_ips, self.request_directory)

        run_list = ["/competition/run_mallob.sh"]
        run_list.append(combined_hostfile)
        run_list.append(problem_path)
        run_list.append(pipeline_mode.name) # forward mode of execution to mallob script

        return run_list

    def get_wc_command(self):
        args = ['/competition/proof_line_count.sh']
        return args

    def get_compress_preprocessing_proof_command(self, input_json):
        args = ['/competition/compress_preprocessing_proof.sh']
        return args

    def get_combine_command(self, input_json):
        problem_path = "/logs/processes/input_units_removed.cnf"
        # D.S.: The proof files are now gathered programmatically within the proof_compose script.
        args = ['/competition/proof_compose.sh', problem_path, '/logs/processes/combined.lrat']
        return args

    def get_renumber_command(self, input_json):
        problem_path = input_json.get("problem_path")
        args = ['/competition/proof_renumber.sh'] # the arguments are currently baked into the script
        return args

    def get_check_command(self, input_json):
        problem_path = input_json.get("problem_path")
        args = ['/competition/proof_check.sh', problem_path]
        return args
    
    def get_drat_combine_command(self, input_json):
        args = ['/competition/drat_compose.sh', input_json.get("problem_path")]
        return args

    def get_drat_check_command(self, input_json):
        args = ['/drat-trim', input_json.get("problem_path"), '/logs/processes/final.drat']
        return args

class MallobParser:
    @staticmethod
    def get_result(output_file, stderr_file):
        """
        TODO: Participants should replace this with something more robust for their own solver!
        """
        with open(output_file) as f:
            timings = []

            raw_logs = f.read()
            if "s UNSATISFIABLE" in raw_logs:
                outcome = "UNSATISFIABLE"
            elif "s SATISFIABLE" in raw_logs:
                outcome = "SATISFIABLE"
            elif "result SAT" in raw_logs:
                outcome = "SATISFIABLE"
            elif "result UNSAT" in raw_logs:
                outcome = "UNSATISFIABLE"
            elif "[ERROR]" in raw_logs:
                outcome = "ERROR"
            else:
                outcome = "UNKNOWN"

            # Some hacks to get all the timings into stderr.
            # Not so proud of this, but it works.

            for elem in re.findall("TIMING (.*) (.*)", raw_logs):
                print(f"I found one!: {elem[0]}, {elem[1]}")
                timings.append((elem[0], elem[1]))

        with open(stderr_file, 'a') as f:
            for (task, time_required) in timings:
                timing_str = f">>>>>{task} : {time_required} seconds<<<<<\n"
                f.write(timing_str)
        
        return outcome

    

def run_sequence(runner, sequence, result, time_remaining):            
    for call in sequence: 
        cmd = call()
        task_output = runner.run(cmd, time_remaining, True)
        time_remaining = time_remaining - task_output["time"]
        return_code = task_output["return_code"]
        if return_code:
            runner.logger.info(f"Cmd: {cmd} return code was non-zero ({return_code}).  Returning 'ERROR'.")
            result = "ERROR"
            break
        elif task_output["timed_out"]:
            runner.logger.info(f"Time-out during cmd {cmd}.  Returning 'UNVERIFIED UNSATISFIABLE'.")
            result = "UNVERIFIED UNSATISFIABLE"
            break    
    return result


def main():
    request_directory = sys.argv[1]
    runner = Runner(request_directory)
    runner.logger.info("Hello from /competition/solver")
    
    input_json = runner.get_input_json()
    cmd = runner.get_command(input_json)
    
    runner.logger.info("Running solver ...")
    time_remaining = TOTAL_TIME

    task_output = runner.run(cmd, time_remaining, False)
    result = MallobParser.get_result(task_output["stdout"], task_output["stderr"])
    return_code = task_output["return_code"]
    runner.logger.info(f"RESULT: {result}")
    time_remaining = time_remaining - task_output["time"]
    if task_output["timed_out"]:
        result = "TIMED_OUT"

    # proof check sequence
    assert pipeline_mode == PipelineMode.SINGLE_MACHINE_64HWT_SEQ_PPROD_AND_DRAT
    if (result == "UNSATISFIABLE"):

        unsat_sequence_0 = [lambda: runner.get_compress_preprocessing_proof_command(input_json)]
        unsat_sequence_par_1 = [lambda: runner.get_combine_command(input_json),
                          lambda: runner.get_renumber_command(input_json), 
                          lambda: runner.get_check_command(input_json),
                          lambda: runner.get_wc_command()]
        unsat_sequence_par_2 = [lambda: runner.get_drat_combine_command(input_json),
                          lambda: runner.get_drat_check_command(input_json)]

        result = run_sequence(runner, unsat_sequence_0, result, time_remaining) 
        executor = ThreadPoolExecutor(2)
        futures = [executor.submit(run_sequence, runner, unsat_sequence_par_1, result, time_remaining), \
                   executor.submit(run_sequence, runner, unsat_sequence_par_2, result, time_remaining)]
        concurrent.futures.wait(futures, timeout=None, return_when=concurrent.futures.ALL_COMPLETED)
        result_seq = futures[0].result()
        result_drat = futures[1].result()

        if (result_seq == result_drat): 
            result = result_seq
        if (result_seq == "ERROR" or result_drat == "ERROR"):
            result = "ERROR"
        elif (result_seq == "UNVERIFIED UNSATISFIABLE" or result_drat == "UNVERIFIED UNSATISFIABLE"):
            result = "UNVERIFIED UNSATISFIABLE"
        else: 
            raise Exception(f"Unexpected combination of results! {result_seq} and {result_drat}")
        
        if result == "UNSATISFIABLE": 
            runner.logger.info(f"Checker return code was zero.  Check successful!")
                    
    # clean up.
    runner.run(['/competition/cleanup'], time_remaining, True)
    
    solver_output = {
        "return_code": return_code,
        "result": result,
        "artifacts": {
            "stdout_path": task_output["stdout"],
            "stderr_path": task_output["stderr"]
        }
    }
    
    with open(os.path.join(request_directory, "solver_out.json"), "w+") as f:
        f.write(json.dumps(solver_output))

if __name__ == "__main__":
    main()
